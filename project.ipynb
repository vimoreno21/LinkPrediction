{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ca91d570",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import urllib.request\n",
    "import tarfile\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split, learning_curve, validation_curve\n",
    "from sklearn.metrics import roc_auc_score, precision_recall_curve, average_precision_score\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1750482",
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_data(url, extract_dir):\n",
    "    \"\"\"\n",
    "    Download and extract the dataset if it doesn't exist already\n",
    "    \"\"\"\n",
    "    if not os.path.exists(extract_dir):\n",
    "        os.makedirs(extract_dir)\n",
    "\n",
    "    # Download the file\n",
    "    tar_file_path = os.path.join(extract_dir, \"ego-twitter.tar.gz\")\n",
    "    if not os.path.exists(tar_file_path):\n",
    "        print(f\"Downloading the dataset from {url}...\")\n",
    "        urllib.request.urlretrieve(url, tar_file_path)\n",
    "\n",
    "    # Extract the file\n",
    "    if not os.path.exists(os.path.join(extract_dir, \"twitter\")):\n",
    "        print(\"Extracting the dataset...\")\n",
    "        with tarfile.open(tar_file_path, \"r:gz\") as tar:\n",
    "            tar.extractall(extract_dir)\n",
    "\n",
    "    print(\"Data download and extraction complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8a4f9d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_network(ego_id, data_dir):\n",
    "    \"\"\"\n",
    "    Load the network data for a specific ego ID\n",
    "    \"\"\"\n",
    "    # Path to edge file\n",
    "    edge_file = os.path.join(data_dir, \"twitter\", f\"{ego_id}.edges\")\n",
    "\n",
    "    # Create a directed graph (since Twitter follows are directed)\n",
    "    G = nx.DiGraph()\n",
    "\n",
    "    # Add edges from the edge file\n",
    "    with open(edge_file, 'r') as f:\n",
    "        for line in f:\n",
    "            source, target = map(int, line.strip().split())\n",
    "            G.add_edge(source, target)\n",
    "\n",
    "    # Add the ego node (assumes ego follows everyone)\n",
    "    ego_node = int(ego_id)\n",
    "    # Create a copy of nodes before modifying the graph\n",
    "    nodes = list(G.nodes())\n",
    "    for node in nodes:\n",
    "        G.add_edge(ego_node, node)\n",
    "\n",
    "    # Load circles if available\n",
    "    circles = {}\n",
    "    circles_file = os.path.join(data_dir, \"twitter\", f\"{ego_id}.circles\")\n",
    "    if os.path.exists(circles_file):\n",
    "        with open(circles_file, 'r') as f:\n",
    "            for line in f:\n",
    "                parts = line.strip().split()\n",
    "                circle_name = parts[0]\n",
    "                circle_members = [int(x) for x in parts[1:]]\n",
    "                circles[circle_name] = circle_members\n",
    "\n",
    "    return G, circles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70c39f56",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_features(ego_id, data_dir):\n",
    "    \"\"\"\n",
    "    Load node features if available\n",
    "    \"\"\"\n",
    "    features = {}\n",
    "    feat_file = os.path.join(data_dir, \"twitter\", f\"{ego_id}.feat\")\n",
    "\n",
    "    if os.path.exists(feat_file):\n",
    "        # Load feature names if available\n",
    "        feature_names = []\n",
    "        featnames_file = os.path.join(data_dir, \"twitter\", f\"{ego_id}.featnames\")\n",
    "        if os.path.exists(featnames_file):\n",
    "            with open(featnames_file, 'r') as f:\n",
    "                for line in f:\n",
    "                    feature_names.append(line.strip().split()[1])\n",
    "\n",
    "        # Load features\n",
    "        with open(feat_file, 'r') as f:\n",
    "            for line in f:\n",
    "                parts = line.strip().split()\n",
    "                node_id = int(parts[0])\n",
    "                node_features = [int(x) for x in parts[1:]]\n",
    "                features[node_id] = node_features\n",
    "\n",
    "    return features, feature_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3347f8d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_topological_features(G):\n",
    "    \"\"\"\n",
    "    Extract topological features for each node in the graph\n",
    "\n",
    "    This function calculates various centrality measures and structural properties\n",
    "    that describe each node's position and importance in the network. These\n",
    "    features will later be used for link prediction.\n",
    "\n",
    "    Args:\n",
    "        G: NetworkX DiGraph - The network graph\n",
    "\n",
    "    Returns:\n",
    "        Dictionary mapping each node to its feature dictionary\n",
    "    \"\"\"\n",
    "    # --- Centrality Measures ---\n",
    "\n",
    "    # Degree centrality: proportion of nodes that this node is connected to\n",
    "    # Higher values indicate nodes that connect to many others\n",
    "    degree_centrality = nx.degree_centrality(G)\n",
    "\n",
    "    # In-degree centrality: proportion of nodes that follow this node\n",
    "    # Higher values indicate popular/influential nodes that many others connect to\n",
    "    in_degree_centrality = nx.in_degree_centrality(G)\n",
    "\n",
    "    # Out-degree centrality: proportion of nodes that this node follows\n",
    "    # Higher values indicate active nodes that connect to many others\n",
    "    out_degree_centrality = nx.out_degree_centrality(G)\n",
    "\n",
    "    # --- Influence Measures ---\n",
    "\n",
    "    # PageRank: measure of node importance based on link structure\n",
    "    # Higher values indicate nodes that are connected to other important nodes\n",
    "    try:\n",
    "        # Alpha is the damping parameter (probability of continuing)\n",
    "        pagerank = nx.pagerank(G, alpha=0.85, max_iter=100)\n",
    "    except:\n",
    "        # Fallback if calculation fails (e.g., for disconnected graphs)\n",
    "        pagerank = {node: 0.0 for node in G.nodes()}\n",
    "\n",
    "    # --- Structural Measures ---\n",
    "\n",
    "    # Clustering coefficient: measure of how connected a node's neighbors are to each other\n",
    "    # Higher values indicate nodes in tightly-knit communities\n",
    "    try:\n",
    "        # Convert to undirected since clustering is an undirected concept\n",
    "        clustering = nx.clustering(G.to_undirected())\n",
    "    except:\n",
    "        # Fallback if calculation fails\n",
    "        clustering = {node: 0.0 for node in G.nodes()}\n",
    "\n",
    "    # --- Combine All Features ---\n",
    "\n",
    "    # Create a dictionary with all features for each node\n",
    "    node_features = {}\n",
    "    for node in G.nodes():\n",
    "        node_features[node] = {\n",
    "            'degree_centrality': degree_centrality.get(node, 0.0),\n",
    "            'in_degree_centrality': in_degree_centrality.get(node, 0.0),\n",
    "            'out_degree_centrality': out_degree_centrality.get(node, 0.0),\n",
    "            'pagerank': pagerank.get(node, 0.0),\n",
    "            'clustering': clustering.get(node, 0.0)\n",
    "        }\n",
    "\n",
    "    return node_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a72d7d7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_edge_features(G, node_u, node_v, node_features):\n",
    "    \"\"\"\n",
    "    Compute features for a potential edge between nodes u and v\n",
    "\n",
    "    This function creates a comprehensive feature vector for a potential connection between\n",
    "    two nodes, combining individual node characteristics and relationship metrics.\n",
    "\n",
    "    Feature Categories:\n",
    "    1. Node-specific features for both endpoints (from node_features)\n",
    "    2. Topological features measuring similarity or connection strength\n",
    "    3. Directed relationship features specific to follower networks\n",
    "\n",
    "    Args:\n",
    "        G: NetworkX DiGraph - The network graph\n",
    "        node_u: Source node ID\n",
    "        node_v: Target node ID\n",
    "        node_features: Dictionary of node features from extract_topological_features()\n",
    "\n",
    "    Returns:\n",
    "        Dictionary of edge features\n",
    "    \"\"\"\n",
    "    # --- 1. Node-specific Features ---\n",
    "\n",
    "    # Get pre-calculated features for both nodes\n",
    "    u_feats = node_features.get(node_u, {})\n",
    "    v_feats = node_features.get(node_v, {})\n",
    "\n",
    "    # --- 2. Topological Connection Features ---\n",
    "\n",
    "    # For topological measures, we use undirected version of the graph\n",
    "    # This captures general connectivity patterns regardless of direction\n",
    "    G_undirected = G.to_undirected()\n",
    "\n",
    "    # Common neighbors: number of shared connections between u and v\n",
    "    # Higher values suggest nodes in the same community or with similar interests\n",
    "    try:\n",
    "        common_neighbors = list(nx.common_neighbors(G_undirected, node_u, node_v))\n",
    "        num_common_neighbors = len(common_neighbors)\n",
    "    except:\n",
    "        num_common_neighbors = 0\n",
    "\n",
    "    # Preferential attachment: product of node degrees\n",
    "    # Based on the idea that highly-connected nodes are more likely to form new connections\n",
    "    pref_attachment = G_undirected.degree(node_u) * G_undirected.degree(node_v)\n",
    "\n",
    "    # Jaccard coefficient: proportion of common neighbors relative to all neighbors\n",
    "    # Normalizes common neighbors by total neighborhood size\n",
    "    try:\n",
    "        u_neighbors = set(G_undirected.neighbors(node_u))\n",
    "        v_neighbors = set(G_undirected.neighbors(node_v))\n",
    "        if len(u_neighbors | v_neighbors) > 0:\n",
    "            jaccard = len(u_neighbors & v_neighbors) / len(u_neighbors | v_neighbors)\n",
    "        else:\n",
    "            jaccard = 0.0\n",
    "    except:\n",
    "        jaccard = 0.0\n",
    "\n",
    "    # --- 3. Directed Relationship Features ---\n",
    "\n",
    "    # Features specific to directed networks like Twitter\n",
    "    try:\n",
    "        # Get followers and followees for both nodes\n",
    "        u_successors = set(G.successors(node_u))  # Users that u follows\n",
    "        u_predecessors = set(G.predecessors(node_u))  # Users that follow u\n",
    "        v_successors = set(G.successors(node_v))  # Users that v follows\n",
    "        v_predecessors = set(G.predecessors(node_v))  # Users that follow v\n",
    "\n",
    "        # Reciprocity: whether v already follows u (indicating potential for reciprocation)\n",
    "        # Binary feature: 1.0 if v follows u, 0.0 otherwise\n",
    "        reciprocity = 1.0 if node_u in v_successors else 0.0\n",
    "\n",
    "        # Follower overlap: proportion of shared followers relative to the smaller follower set\n",
    "        # Higher values indicate users with similar audiences\n",
    "        follower_overlap = len(u_predecessors & v_predecessors) / max(1, min(len(u_predecessors), len(v_predecessors)))\n",
    "\n",
    "        # Following overlap: proportion of shared followees relative to the smaller following set\n",
    "        # Higher values indicate users with similar interests\n",
    "        following_overlap = len(u_successors & v_successors) / max(1, min(len(u_successors), len(v_successors)))\n",
    "    except:\n",
    "        # Default values if calculation fails\n",
    "        reciprocity = 0.0\n",
    "        follower_overlap = 0.0\n",
    "        following_overlap = 0.0\n",
    "\n",
    "    # Adamic-Adar index: weighted common neighbors (higher weight for rare connections)\n",
    "    # Gives more importance to common neighbors that have fewer connections themselves\n",
    "    adamic_adar = 0.0\n",
    "    try:\n",
    "        for common_neighbor in common_neighbors:\n",
    "            neighbor_degree = G_undirected.degree(common_neighbor)\n",
    "            if neighbor_degree > 1:  # Avoid log(1) = 0 division\n",
    "                adamic_adar += 1.0 / np.log(neighbor_degree)\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    # --- Combine All Features Into a Single Vector ---\n",
    "\n",
    "    edge_features = {\n",
    "        # Node features for source node (u)\n",
    "        'u_degree': u_feats.get('degree_centrality', 0.0),\n",
    "        'u_in_degree': u_feats.get('in_degree_centrality', 0.0),\n",
    "        'u_out_degree': u_feats.get('out_degree_centrality', 0.0),\n",
    "        'u_pagerank': u_feats.get('pagerank', 0.0),\n",
    "        'u_clustering': u_feats.get('clustering', 0.0),\n",
    "\n",
    "        # Node features for target node (v)\n",
    "        'v_degree': v_feats.get('degree_centrality', 0.0),\n",
    "        'v_in_degree': v_feats.get('in_degree_centrality', 0.0),\n",
    "        'v_out_degree': v_feats.get('out_degree_centrality', 0.0),\n",
    "        'v_pagerank': v_feats.get('pagerank', 0.0),\n",
    "        'v_clustering': v_feats.get('clustering', 0.0),\n",
    "\n",
    "        # Edge-specific topological features\n",
    "        'common_neighbors': num_common_neighbors,\n",
    "        'preferential_attachment': pref_attachment,\n",
    "        'jaccard_coefficient': jaccard,\n",
    "        'adamic_adar': adamic_adar,\n",
    "\n",
    "        # Directed relationship features\n",
    "        'reciprocity': reciprocity,\n",
    "        'follower_overlap': follower_overlap,\n",
    "        'following_overlap': following_overlap\n",
    "    }\n",
    "\n",
    "    return edge_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb823091",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_link_prediction_data(G, test_ratio=0.3, neg_ratio=1.0):\n",
    "    \"\"\"\n",
    "    Prepare data for link prediction by splitting the existing edges into training and testing sets,\n",
    "    and sampling non-existing edges as negative examples.\n",
    "\n",
    "    Data Splitting Process:\n",
    "    1. Existing edges are randomly split into training (70%) and testing (30%) sets\n",
    "    2. Test edges are removed from the training graph to ensure they're unseen during training\n",
    "    3. Equal numbers of non-existing edges are sampled as negative examples\n",
    "    4. Training negative edges are sampled from the full graph\n",
    "    5. Testing negative edges are sampled such that they don't overlap with training negatives\n",
    "\n",
    "    Args:\n",
    "        G: NetworkX DiGraph - The full network graph\n",
    "        test_ratio: Fraction of edges to use for testing (default: 0.3 or 30%)\n",
    "        neg_ratio: Ratio of negative samples to positive samples (default: 1.0 = equal numbers)\n",
    "\n",
    "    Returns:\n",
    "        G_train: Graph with only training edges (test edges removed)\n",
    "        train_edges: List of edges for training (positive examples)\n",
    "        test_edges: List of edges for testing (positive examples)\n",
    "        train_non_edges: List of non-existing edges for training (negative examples)\n",
    "        test_non_edges: List of non-existing edges for testing (negative examples)\n",
    "    \"\"\"\n",
    "    # Step 1: Extract all existing edges from the graph\n",
    "    all_edges = list(G.edges())\n",
    "\n",
    "    # Step 2: Split existing edges into training and testing sets\n",
    "    # This uses scikit-learn's train_test_split with fixed random state for reproducibility\n",
    "    # By default: 70% for training, 30% for testing\n",
    "    train_edges, test_edges = train_test_split(all_edges, test_size=test_ratio, random_state=42)\n",
    "\n",
    "    # Step 3: Create a new graph with only training edges\n",
    "    # This is critical - we remove test edges from the training graph\n",
    "    # to ensure our model doesn't \"see\" connections it will later need to predict\n",
    "    G_train = G.copy()\n",
    "    G_train.remove_edges_from(test_edges)\n",
    "\n",
    "    # Step 4: Get all nodes for negative edge sampling\n",
    "    nodes = list(G.nodes())\n",
    "    n_nodes = len(nodes)\n",
    "\n",
    "    # Helper function to sample negative edges (non-existing connections)\n",
    "    def sample_non_edges(graph, num_samples, exclude_edges=None):\n",
    "        \"\"\"\n",
    "        Sample random node pairs that don't have an edge between them\n",
    "\n",
    "        Args:\n",
    "            graph: The graph to sample from\n",
    "            num_samples: Number of negative edges to sample\n",
    "            exclude_edges: Set of edges to exclude from sampling (to prevent overlap)\n",
    "\n",
    "        Returns:\n",
    "            List of sampled non-edges (node pairs)\n",
    "        \"\"\"\n",
    "        # Initialize exclusion set to avoid duplicate sampling\n",
    "        if exclude_edges is None:\n",
    "            exclude_edges = set()\n",
    "        else:\n",
    "            exclude_edges = set(exclude_edges)\n",
    "\n",
    "        non_edges = []\n",
    "        # Keep sampling until we reach the required number\n",
    "        while len(non_edges) < num_samples:\n",
    "            # Randomly select two distinct nodes\n",
    "            u, v = random.sample(nodes, 2)\n",
    "\n",
    "            # Only add pair if:\n",
    "            # 1. Nodes are different (no self-loops)\n",
    "            # 2. No edge exists between them in the graph\n",
    "            # 3. This pair hasn't already been sampled (not in exclude_edges)\n",
    "            if u != v and not graph.has_edge(u, v) and (u, v) not in exclude_edges:\n",
    "                non_edges.append((u, v))\n",
    "                exclude_edges.add((u, v))  # Add to exclusion set to prevent resampling\n",
    "\n",
    "        return non_edges\n",
    "\n",
    "    # Step 5: Calculate how many negative edges to sample based on positive examples\n",
    "    n_pos_train = len(train_edges)\n",
    "    n_pos_test = len(test_edges)\n",
    "\n",
    "    # Step 6: Sample negative edges for training and testing sets\n",
    "    # Sample negatives for training set\n",
    "    train_non_edges = sample_non_edges(G, int(n_pos_train * neg_ratio))\n",
    "\n",
    "    # Sample negatives for testing set, ensuring no overlap with training negatives\n",
    "    test_non_edges = sample_non_edges(G, int(n_pos_test * neg_ratio), exclude_edges=train_non_edges)\n",
    "\n",
    "    # Return the training graph and all edge sets\n",
    "    return G_train, train_edges, test_edges, train_non_edges, test_non_edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d38821e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_edge_dataset(G, pos_edges, neg_edges, node_features):\n",
    "    \"\"\"\n",
    "    Create a machine learning dataset for edge prediction\n",
    "\n",
    "    This function transforms network edges into feature vectors suitable for\n",
    "    machine learning algorithms. It processes both positive examples (existing edges)\n",
    "    and negative examples (non-existing edges) to create a balanced dataset.\n",
    "\n",
    "    Dataset Creation Process:\n",
    "    1. For each positive edge (existing connection), compute feature vector and assign label 1\n",
    "    2. For each negative edge (non-connection), compute feature vector and assign label 0\n",
    "    3. Combine all examples into feature matrix X and label vector y\n",
    "\n",
    "    Args:\n",
    "        G: NetworkX DiGraph - The network graph\n",
    "        pos_edges: List of positive edges (tuples of node pairs)\n",
    "        neg_edges: List of negative edges (tuples of node pairs)\n",
    "        node_features: Dictionary of node features from extract_topological_features()\n",
    "\n",
    "    Returns:\n",
    "        X: numpy.ndarray - Feature matrix where each row is an edge's feature vector\n",
    "        y: numpy.ndarray - Label vector (1 for positive edges, 0 for negative edges)\n",
    "    \"\"\"\n",
    "    X = []  # Will hold feature vectors\n",
    "    y = []  # Will hold labels\n",
    "\n",
    "    # --- Process Positive Examples ---\n",
    "\n",
    "    # For each existing edge, compute features and assign positive label\n",
    "    print(f\"Processing {len(pos_edges)} positive examples...\")\n",
    "    for u, v in pos_edges:\n",
    "        # Compute feature vector for this edge\n",
    "        edge_feats = compute_edge_features(G, u, v, node_features)\n",
    "\n",
    "        # Add feature vector and positive label\n",
    "        X.append(list(edge_feats.values()))\n",
    "        y.append(1)  # Positive class (edge exists)\n",
    "\n",
    "    # --- Process Negative Examples ---\n",
    "\n",
    "    # For each non-existing edge, compute features and assign negative label\n",
    "    print(f\"Processing {len(neg_edges)} negative examples...\")\n",
    "    for u, v in neg_edges:\n",
    "        # Compute feature vector for this non-edge\n",
    "        edge_feats = compute_edge_features(G, u, v, node_features)\n",
    "\n",
    "        # Add feature vector and negative label\n",
    "        X.append(list(edge_feats.values()))\n",
    "        y.append(0)  # Negative class (edge doesn't exist)\n",
    "\n",
    "    # Convert lists to numpy arrays for machine learning algorithms\n",
    "    return np.array(X), np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a338b530",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(y_true, y_scores):\n",
    "    \"\"\"\n",
    "    Evaluate model performance using classification metrics\n",
    "\n",
    "    This function calculates and visualizes key performance metrics for the link prediction model:\n",
    "    - ROC AUC: Area under the Receiver Operating Characteristic curve\n",
    "    - Average Precision: Area under the Precision-Recall curve\n",
    "\n",
    "    Args:\n",
    "        y_true: numpy.ndarray - Ground truth labels (1 for positive edges, 0 for negative edges)\n",
    "        y_scores: numpy.ndarray - Predicted probability scores from the model\n",
    "\n",
    "    Returns:\n",
    "        Tuple containing (auc_score, ap_score)\n",
    "    \"\"\"\n",
    "    # ---------- CLASSIFICATION METRICS ----------\n",
    "\n",
    "    # ROC AUC score - measures the model's ability to discriminate between classes\n",
    "    # Higher values indicate better separation between positive and negative examples\n",
    "    # 0.5 = random guessing, 1.0 = perfect separation\n",
    "    auc_score = roc_auc_score(y_true, y_scores)\n",
    "\n",
    "    # Precision-Recall curve data points\n",
    "    precision, recall, thresholds = precision_recall_curve(y_true, y_scores)\n",
    "\n",
    "    # Average Precision - summarizes the precision-recall curve as the weighted mean of precisions at each threshold\n",
    "    # Higher values indicate better performance across different threshold settings\n",
    "    ap_score = average_precision_score(y_true, y_scores)\n",
    "\n",
    "    # Print the metrics\n",
    "    print(f\"Performance metrics:\")\n",
    "    print(f\"  - ROC AUC Score: {auc_score:.4f}\")\n",
    "    print(f\"  - Average Precision Score: {ap_score:.4f}\")\n",
    "\n",
    "    # ---------- VISUALIZATION: PRECISION-RECALL CURVE ----------\n",
    "\n",
    "    # Plot Precision-Recall curve\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(recall, precision, marker='.', label=f'Average Precision = {ap_score:.4f}')\n",
    "\n",
    "    # Add a baseline (random classifier)\n",
    "    # For a balanced dataset, this would be the positive class proportion\n",
    "    positive_ratio = np.mean(y_true)\n",
    "    plt.axhline(y=positive_ratio, color='r', linestyle='--', alpha=0.5,\n",
    "               label=f'Random Classifier (AP = {positive_ratio:.4f})')\n",
    "\n",
    "    # Add chart decorations\n",
    "    plt.xlabel('Recall')\n",
    "    plt.ylabel('Precision')\n",
    "    plt.title('Precision-Recall Curve for Link Prediction')\n",
    "    plt.legend(loc='best')\n",
    "    plt.grid(alpha=0.3)\n",
    "\n",
    "    # Save the figure\n",
    "    plt.savefig(\"precision_recall_curve.png\", dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "    # ---------- VISUALIZATION: ROC CURVE ----------\n",
    "\n",
    "    from sklearn.metrics import roc_curve\n",
    "\n",
    "    # Calculate ROC curve\n",
    "    fpr, tpr, _ = roc_curve(y_true, y_scores)\n",
    "\n",
    "    # Plot ROC curve\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(fpr, tpr, label=f'ROC Curve (AUC = {auc_score:.4f})')\n",
    "\n",
    "    # Add diagonal line (random classifier)\n",
    "    plt.plot([0, 1], [0, 1], 'k--', alpha=0.5, label='Random Classifier (AUC = 0.5)')\n",
    "\n",
    "    # Add chart decorations\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title('ROC Curve for Link Prediction')\n",
    "    plt.legend(loc='best')\n",
    "    plt.grid(alpha=0.3)\n",
    "\n",
    "    # Save the figure\n",
    "    plt.savefig(\"roc_curve.png\", dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "    # ---------- VISUALIZATION: THRESHOLD ANALYSIS ----------\n",
    "\n",
    "    # Create a dataframe with different threshold values and their corresponding metrics\n",
    "    threshold_metrics = []\n",
    "\n",
    "    # Exclude the last threshold which is often 0 and leads to division by zero\n",
    "    for i in range(len(thresholds)):\n",
    "        # Predictions at current threshold\n",
    "        y_pred = (y_scores >= thresholds[i]).astype(int)\n",
    "\n",
    "        # Calculate metrics\n",
    "        true_positives = np.sum((y_pred == 1) & (y_true == 1))\n",
    "        false_positives = np.sum((y_pred == 1) & (y_true == 0))\n",
    "        true_negatives = np.sum((y_pred == 0) & (y_true == 0))\n",
    "        false_negatives = np.sum((y_pred == 0) & (y_true == 1))\n",
    "\n",
    "        # Handle division by zero\n",
    "        if true_positives + false_positives == 0:\n",
    "            prec = 0\n",
    "        else:\n",
    "            prec = true_positives / (true_positives + false_positives)\n",
    "\n",
    "        if true_positives + false_negatives == 0:\n",
    "            rec = 0\n",
    "        else:\n",
    "            rec = true_positives / (true_positives + false_negatives)\n",
    "\n",
    "        # Calculate F1 score\n",
    "        if prec + rec == 0:\n",
    "            f1 = 0\n",
    "        else:\n",
    "            f1 = 2 * (prec * rec) / (prec + rec)\n",
    "\n",
    "        # Calculate accuracy\n",
    "        acc = (true_positives + true_negatives) / len(y_true)\n",
    "\n",
    "        threshold_metrics.append({\n",
    "            'Threshold': thresholds[i],\n",
    "            'Precision': prec,\n",
    "            'Recall': rec,\n",
    "            'F1 Score': f1,\n",
    "            'Accuracy': acc,\n",
    "            'True Positives': true_positives,\n",
    "            'False Positives': false_positives\n",
    "        })\n",
    "\n",
    "    # Convert to dataframe\n",
    "    threshold_df = pd.DataFrame(threshold_metrics)\n",
    "\n",
    "    # Plot metrics vs threshold\n",
    "    plt.figure(figsize=(12, 8))\n",
    "\n",
    "    plt.plot(threshold_df['Threshold'], threshold_df['Precision'], 'b-', label='Precision')\n",
    "    plt.plot(threshold_df['Threshold'], threshold_df['Recall'], 'g-', label='Recall')\n",
    "    plt.plot(threshold_df['Threshold'], threshold_df['F1 Score'], 'r-', label='F1 Score')\n",
    "    plt.plot(threshold_df['Threshold'], threshold_df['Accuracy'], 'y-', label='Accuracy')\n",
    "\n",
    "    # Add chart decorations\n",
    "    plt.xlabel('Threshold')\n",
    "    plt.ylabel('Score')\n",
    "    plt.title('Metric Performance vs. Classification Threshold')\n",
    "    plt.legend(loc='best')\n",
    "    plt.grid(alpha=0.3)\n",
    "\n",
    "    # Save the figure\n",
    "    plt.savefig(\"threshold_analysis.png\", dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "    return auc_score, ap_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0bdb7fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_link_prediction(ego_id, data_dir):\n",
    "    \"\"\"\n",
    "    Run the complete link prediction pipeline for a given ego network\n",
    "\n",
    "    This is the main function that orchestrates the entire machine learning workflow:\n",
    "    1. Data loading and preprocessing\n",
    "    2. Feature extraction\n",
    "    3. Train/test splitting\n",
    "    4. Model training and evaluation\n",
    "    5. Visualization of results\n",
    "\n",
    "    Args:\n",
    "        ego_id: ID of the ego network to analyze\n",
    "        data_dir: Directory containing the dataset\n",
    "\n",
    "    Returns:\n",
    "        Dictionary containing performance metrics and feature importance\n",
    "    \"\"\"\n",
    "    print(f\"\\n======= Running Link Prediction for Ego Network {ego_id} =======\")\n",
    "\n",
    "    # ---------- STEP 1: DATA LOADING ----------\n",
    "\n",
    "    print(\"\\n[1/7] Loading network data...\")\n",
    "    # Load the network structure (nodes and edges)\n",
    "    G, circles = load_network(ego_id, data_dir)\n",
    "\n",
    "    # Load profile features if available (not used in this implementation)\n",
    "    node_attrs, feature_names = load_features(ego_id, data_dir)\n",
    "\n",
    "    print(f\"Network loaded: {G.number_of_nodes()} nodes, {G.number_of_edges()} edges\")\n",
    "    if circles:\n",
    "        print(f\"Network contains {len(circles)} circles (communities)\")\n",
    "\n",
    "    # ---------- STEP 2: FEATURE EXTRACTION ----------\n",
    "\n",
    "    print(\"\\n[2/7] Extracting topological features for each node...\")\n",
    "    # Calculate network metrics for each node\n",
    "    topo_features = extract_topological_features(G)\n",
    "    print(f\"Extracted features for {len(topo_features)} nodes\")\n",
    "\n",
    "    # ---------- STEP 3: DATA SPLITTING ----------\n",
    "\n",
    "    print(\"\\n[3/7] Splitting data into training and testing sets...\")\n",
    "    # Split existing edges into train/test and sample non-edges\n",
    "    G_train, train_edges, test_edges, train_non_edges, test_non_edges = prepare_link_prediction_data(G)\n",
    "\n",
    "    # Data splitting summary\n",
    "    print(f\"\\nData splitting summary:\")\n",
    "    print(f\"  - Full network: {G.number_of_edges()} edges\")\n",
    "    print(f\"  - Training network: {G_train.number_of_edges()} edges ({G_train.number_of_edges()/G.number_of_edges()*100:.1f}%)\")\n",
    "    print(f\"  - Training set: {len(train_edges)} positive edges, {len(train_non_edges)} negative edges\")\n",
    "    print(f\"  - Testing set: {len(test_edges)} positive edges, {len(test_non_edges)} negative edges\")\n",
    "\n",
    "    # ---------- STEP 4: FEATURE VECTOR CREATION ----------\n",
    "\n",
    "    print(\"\\n[4/7] Creating feature vectors for edges...\")\n",
    "    # Create feature matrices for training and testing\n",
    "    X_train, y_train = create_edge_dataset(G_train, train_edges, train_non_edges, topo_features)\n",
    "    X_test, y_test = create_edge_dataset(G_train, test_edges, test_non_edges, topo_features)\n",
    "\n",
    "    print(f\"Created {len(X_train)} training samples and {len(X_test)} testing samples\")\n",
    "    print(f\"Each edge has {X_train.shape[1]} features\")\n",
    "\n",
    "    # ---------- STEP 5: FEATURE SCALING ----------\n",
    "\n",
    "    print(\"\\n[5/7] Scaling features...\")\n",
    "    # Standardize features by removing the mean and scaling to unit variance\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "    # ---------- STEP 6: MODEL TRAINING ----------\n",
    "\n",
    "    print(\"\\n[6/7] Training Random Forest classifier...\")\n",
    "\n",
    "    # --- 6.1: Analyze performance across different numbers of trees ---\n",
    "    print(\"  Analyzing performance with different numbers of trees...\")\n",
    "\n",
    "    # Number of trees to try\n",
    "    n_estimators_list = [1, 5, 10, 25, 50, 75, 100, 150, 200]\n",
    "\n",
    "    # Store results for plotting\n",
    "    train_scores = []\n",
    "    test_scores = []\n",
    "    oob_scores = []\n",
    "    training_times = []\n",
    "\n",
    "    for n_trees in n_estimators_list:\n",
    "        start_time = time.time()\n",
    "\n",
    "        # Train Random Forest with current number of trees\n",
    "        # Include out-of-bag score calculation\n",
    "        rf = RandomForestClassifier(n_estimators=n_trees,\n",
    "                                    oob_score=True,\n",
    "                                    n_jobs=-1,  # Use all available processors\n",
    "                                    random_state=42)\n",
    "\n",
    "        rf.fit(X_train_scaled, y_train)\n",
    "\n",
    "        # Record training time\n",
    "        training_time = time.time() - start_time\n",
    "        training_times.append(training_time)\n",
    "\n",
    "        # Calculate scores\n",
    "        train_score = rf.score(X_train_scaled, y_train)\n",
    "        test_score = rf.score(X_test_scaled, y_test)\n",
    "        oob_score = rf.oob_score_\n",
    "\n",
    "        # Store scores\n",
    "        train_scores.append(train_score)\n",
    "        test_scores.append(test_score)\n",
    "        oob_scores.append(oob_score)\n",
    "\n",
    "        print(f\"    Trees: {n_trees}, Train: {train_score:.4f}, Test: {test_score:.4f}, OOB: {oob_score:.4f}, Time: {training_time:.2f}s\")\n",
    "\n",
    "    # --- 6.2: Visualize performance vs. number of trees ---\n",
    "    plt.figure(figsize=(12, 8))\n",
    "\n",
    "    # Plot accuracy curves\n",
    "    plt.subplot(2, 1, 1)\n",
    "    plt.plot(n_estimators_list, train_scores, 'o-', label='Training Accuracy')\n",
    "    plt.plot(n_estimators_list, test_scores, 'o-', label='Testing Accuracy')\n",
    "    plt.plot(n_estimators_list, oob_scores, 'o-', label='Out-of-Bag Accuracy')\n",
    "\n",
    "    plt.xlabel('Number of Trees')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.title('Random Forest Performance vs. Number of Trees')\n",
    "    plt.grid(alpha=0.3)\n",
    "    plt.legend()\n",
    "\n",
    "    # Plot training time\n",
    "    plt.subplot(2, 1, 2)\n",
    "    plt.plot(n_estimators_list, training_times, 'o-', color='red')\n",
    "    plt.xlabel('Number of Trees')\n",
    "    plt.ylabel('Training Time (seconds)')\n",
    "    plt.title('Training Time vs. Number of Trees')\n",
    "    plt.grid(alpha=0.3)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"rf_trees_performance_ego_{ego_id}.png\", dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "    # --- 6.3: Analyze feature importance stability across different forests ---\n",
    "    print(\"  Analyzing feature importance stability...\")\n",
    "\n",
    "    # Train multiple forests and track feature importance\n",
    "    n_iterations = 10\n",
    "    feature_names = list(compute_edge_features(G, list(G.nodes())[0], list(G.nodes())[1], topo_features).keys())\n",
    "    importance_matrix = np.zeros((n_iterations, len(feature_names)))\n",
    "\n",
    "    for i in range(n_iterations):\n",
    "        # Train model with different random seed\n",
    "        rf = RandomForestClassifier(n_estimators=100, random_state=i)\n",
    "        rf.fit(X_train_scaled, y_train)\n",
    "\n",
    "        # Store feature importance\n",
    "        importance_matrix[i, :] = rf.feature_importances_\n",
    "\n",
    "    # Calculate mean and standard deviation of importance\n",
    "    mean_importance = np.mean(importance_matrix, axis=0)\n",
    "    std_importance = np.std(importance_matrix, axis=0)\n",
    "\n",
    "    # Sort features by mean importance\n",
    "    sorted_idx = np.argsort(mean_importance)\n",
    "    sorted_feature_names = [feature_names[i] for i in sorted_idx]\n",
    "\n",
    "    # Plot feature importance with error bars\n",
    "    plt.figure(figsize=(12, 10))\n",
    "    y_pos = np.arange(len(sorted_feature_names))\n",
    "\n",
    "    plt.barh(y_pos, mean_importance[sorted_idx], xerr=std_importance[sorted_idx],\n",
    "             align='center', alpha=0.7, capsize=5)\n",
    "    plt.yticks(y_pos, sorted_feature_names)\n",
    "    plt.xlabel('Mean Feature Importance')\n",
    "    plt.title('Feature Importance Stability Across Multiple Random Forests')\n",
    "    plt.grid(alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"feature_importance_stability_ego_{ego_id}.png\", dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "    # --- 6.4: Train final model with optimal parameters ---\n",
    "    print(\"  Training final model with 100 trees...\")\n",
    "\n",
    "    # Train the final model\n",
    "    final_rf = RandomForestClassifier(\n",
    "        n_estimators=100,      # Number of trees in the forest\n",
    "        max_depth=None,        # Maximum depth of the trees\n",
    "        min_samples_split=2,   # Minimum samples required to split an internal node\n",
    "        min_samples_leaf=1,    # Minimum samples required at a leaf node\n",
    "        random_state=42        # Random seed for reproducibility\n",
    "    )\n",
    "\n",
    "    # Fit the model on the training data\n",
    "    final_rf.fit(X_train_scaled, y_train)\n",
    "\n",
    "    # Generate predictions (probability scores)\n",
    "    y_pred_proba = final_rf.predict_proba(X_test_scaled)[:, 1]\n",
    "\n",
    "    # --- 6.5: Visualize a sample tree from the forest ---\n",
    "    print(\"  Generating visualization of a sample decision tree...\")\n",
    "\n",
    "    # Get a single tree from the forest\n",
    "    estimator = final_rf.estimators_[0]\n",
    "\n",
    "    from sklearn.tree import export_graphviz\n",
    "    import graphviz\n",
    "\n",
    "    # Export tree to DOT format\n",
    "    dot_data = export_graphviz(\n",
    "        estimator,\n",
    "        out_file=None,\n",
    "        feature_names=feature_names,\n",
    "        class_names=['No Link', 'Link'],\n",
    "        filled=True,\n",
    "        rounded=True,\n",
    "        special_characters=True,\n",
    "        max_depth=3  # Limit depth for visualization\n",
    "    )\n",
    "\n",
    "    # Save the tree visualization\n",
    "    graph = graphviz.Source(dot_data)\n",
    "    graph.render(f\"sample_tree_ego_{ego_id}\")\n",
    "\n",
    "    # ---------- STEP 7: MODEL EVALUATION ----------\n",
    "\n",
    "    print(\"\\n[7/7] Evaluating model performance...\")\n",
    "    # Calculate performance metrics and create visualizations\n",
    "    auc_score, ap_score = evaluate_model(y_test, y_pred_proba)\n",
    "\n",
    "    # Get feature importance from final model\n",
    "    feature_importance = final_rf.feature_importances_\n",
    "\n",
    "    # Return results\n",
    "    return {\n",
    "        'auc_score': auc_score,\n",
    "        'ap_score': ap_score,\n",
    "        'feature_importance': dict(zip(feature_names, feature_importance)),\n",
    "        'train_scores': train_scores,\n",
    "        'test_scores': test_scores,\n",
    "        'oob_scores': oob_scores,\n",
    "        'n_estimators_list': n_estimators_list\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "524b43dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function will visualize the learning curve (training vs testing performance)\n",
    "def plot_learning_curve(estimator, X, y, title=\"Learning Curve\", ylim=None, cv=None,\n",
    "                        n_jobs=None, train_sizes=np.linspace(.1, 1.0, 10)):\n",
    "    \"\"\"\n",
    "    Generate a plot of the learning curve showing model performance as training set size increases.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    estimator : object type that implements the \"fit\" and \"predict\" methods\n",
    "        An object of that type which is cloned for each validation.\n",
    "    X : array-like, shape (n_samples, n_features)\n",
    "        Training vector, where n_samples is the number of samples and n_features is the number of features.\n",
    "    y : array-like, shape (n_samples) or (n_samples, n_features), optional\n",
    "        Target relative to X for classification or regression.\n",
    "    title : string, optional\n",
    "        Title for the chart.\n",
    "    ylim : tuple, optional\n",
    "        Defines minimum and maximum y-values plotted.\n",
    "    cv : int, cross-validation generator or an iterable, optional\n",
    "        Determines the cross-validation splitting strategy.\n",
    "    n_jobs : int or None, optional (default=None)\n",
    "        Number of jobs to run in parallel.\n",
    "    train_sizes : array-like, shape (n_ticks,), dtype float or int\n",
    "        Relative or absolute numbers of training examples for the learning curve points.\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.title(title)\n",
    "    if ylim is not None:\n",
    "        plt.ylim(*ylim)\n",
    "    plt.xlabel(\"Training examples\")\n",
    "    plt.ylabel(\"Score\")\n",
    "\n",
    "    # Calculate learning curve values\n",
    "    train_sizes, train_scores, test_scores = learning_curve(\n",
    "        estimator, X, y, cv=cv, n_jobs=n_jobs, train_sizes=train_sizes)\n",
    "\n",
    "    # Calculate mean and standard deviation for training set scores\n",
    "    train_scores_mean = np.mean(train_scores, axis=1)\n",
    "    train_scores_std = np.std(train_scores, axis=1)\n",
    "\n",
    "    # Calculate mean and standard deviation for test set scores\n",
    "    test_scores_mean = np.mean(test_scores, axis=1)\n",
    "    test_scores_std = np.std(test_scores, axis=1)\n",
    "\n",
    "    # Plot training and test score curves\n",
    "    plt.grid()\n",
    "    plt.fill_between(train_sizes, train_scores_mean - train_scores_std,\n",
    "                     train_scores_mean + train_scores_std, alpha=0.1, color=\"r\")\n",
    "    plt.fill_between(train_sizes, test_scores_mean - test_scores_std,\n",
    "                     test_scores_mean + test_scores_std, alpha=0.1, color=\"g\")\n",
    "    plt.plot(train_sizes, train_scores_mean, 'o-', color=\"r\", label=\"Training score\")\n",
    "    plt.plot(train_sizes, test_scores_mean, 'o-', color=\"g\", label=\"Cross-validation score\")\n",
    "\n",
    "    plt.legend(loc=\"best\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"{title.replace(' ', '_')}.png\", dpi=300, bbox_inches='tight')\n",
    "    return plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4be6b9cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function will visualize how the number of trees affects model performance\n",
    "def plot_n_estimators_curve(X_train, y_train, X_test, y_test, max_n_estimators=200, step=10):\n",
    "    \"\"\"\n",
    "    Plots the effect of increasing the number of trees (n_estimators) in a Random Forest\n",
    "    on both training and testing accuracy.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    X_train : array-like\n",
    "        Training feature matrix\n",
    "    y_train : array-like\n",
    "        Training target vector\n",
    "    X_test : array-like\n",
    "        Testing feature matrix\n",
    "    y_test : array-like\n",
    "        Testing target vector\n",
    "    max_n_estimators : int, optional (default=200)\n",
    "        Maximum number of trees to evaluate\n",
    "    step : int, optional (default=10)\n",
    "        Step size for n_estimators evaluation\n",
    "    \"\"\"\n",
    "    n_estimators_range = range(1, max_n_estimators + 1, step)\n",
    "    train_scores = []\n",
    "    test_scores = []\n",
    "    test_auc_scores = []\n",
    "\n",
    "    # For each number of estimators, train a model and record scores\n",
    "    print(\"Evaluating model performance for different numbers of trees...\")\n",
    "    for n_est in tqdm(n_estimators_range):\n",
    "        # Train model with current number of trees\n",
    "        rf = RandomForestClassifier(n_estimators=n_est, random_state=42)\n",
    "        rf.fit(X_train, y_train)\n",
    "\n",
    "        # Record training accuracy\n",
    "        y_train_pred = rf.predict(X_train)\n",
    "        train_acc = accuracy_score(y_train, y_train_pred)\n",
    "        train_scores.append(train_acc)\n",
    "\n",
    "        # Record testing accuracy and AUC\n",
    "        y_test_pred = rf.predict(X_test)\n",
    "        test_acc = accuracy_score(y_test, y_test_pred)\n",
    "        test_scores.append(test_acc)\n",
    "\n",
    "        # Record AUC score\n",
    "        y_test_proba = rf.predict_proba(X_test)[:, 1]\n",
    "        test_auc = roc_auc_score(y_test, y_test_proba)\n",
    "        test_auc_scores.append(test_auc)\n",
    "\n",
    "    # Plot accuracy vs number of trees\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(n_estimators_range, train_scores, 'o-', color='r', label='Training Accuracy')\n",
    "    plt.plot(n_estimators_range, test_scores, 'o-', color='g', label='Testing Accuracy')\n",
    "    plt.title('Accuracy vs Number of Trees')\n",
    "    plt.xlabel('Number of Trees')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.grid(alpha=0.3)\n",
    "    plt.legend()\n",
    "\n",
    "    # Plot AUC vs number of trees\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(n_estimators_range, test_auc_scores, 'o-', color='b', label='Testing AUC')\n",
    "    plt.title('AUC vs Number of Trees')\n",
    "    plt.xlabel('Number of Trees')\n",
    "    plt.ylabel('AUC Score')\n",
    "    plt.grid(alpha=0.3)\n",
    "    plt.legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "   # plt.savefig(\"Random_Forest_Trees_Performance.png\", dpi=300, bbox_inches='tight')\n",
    "\n",
    "    # Find optimal number of trees\n",
    "    best_idx = np.argmax(test_auc_scores)\n",
    "    optimal_trees = n_estimators_range[best_idx]\n",
    "    best_auc = test_auc_scores[best_idx]\n",
    "\n",
    "    print(f\"Optimal number of trees: {optimal_trees} (AUC: {best_auc:.4f})\")\n",
    "    return optimal_trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77bf769e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function will visualize feature importance and how it changes with trees\n",
    "def plot_feature_importance_evolution(X_train, y_train, feature_names, max_trees=100, intervals=5):\n",
    "    \"\"\"\n",
    "    Visualizes how feature importance evolves as the number of trees increases in a Random Forest.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    X_train : array-like\n",
    "        Training feature matrix\n",
    "    y_train : array-like\n",
    "        Training target vector\n",
    "    feature_names : list\n",
    "        Names of features\n",
    "    max_trees : int, optional (default=100)\n",
    "        Maximum number of trees to evaluate\n",
    "    intervals : int, optional (default=5)\n",
    "        Number of points to evaluate\n",
    "    \"\"\"\n",
    "    # Generate evenly spaced tree counts\n",
    "    tree_counts = np.linspace(5, max_trees, intervals, dtype=int)\n",
    "\n",
    "    # Initialize importance matrix\n",
    "    n_features = len(feature_names)\n",
    "    importance_matrix = np.zeros((len(tree_counts), n_features))\n",
    "\n",
    "    # For each tree count, train model and record feature importances\n",
    "    for i, n_trees in enumerate(tree_counts):\n",
    "        rf = RandomForestClassifier(n_estimators=n_trees, random_state=42)\n",
    "        rf.fit(X_train, y_train)\n",
    "        importance_matrix[i, :] = rf.feature_importances_\n",
    "\n",
    "    # Convert to DataFrame for easier visualization\n",
    "    importance_df = pd.DataFrame(importance_matrix,\n",
    "                               columns=feature_names,\n",
    "                               index=[f\"{n} trees\" for n in tree_counts])\n",
    "\n",
    "    # Get top 10 most important features (from final model)\n",
    "    top_features_idx = np.argsort(importance_matrix[-1])[-10:]\n",
    "    top_features = [feature_names[i] for i in top_features_idx]\n",
    "\n",
    "    # Plot heatmap of feature importance evolution for top features\n",
    "    plt.figure(figsize=(14, 8))\n",
    "    sns.heatmap(importance_df[top_features].T, annot=True, cmap='viridis', fmt='.3f')\n",
    "    plt.title('Feature Importance Evolution with Increasing Trees')\n",
    "    plt.xlabel('Number of Trees in Random Forest')\n",
    "    plt.ylabel('Features')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\"Feature_Importance_Evolution.png\", dpi=300, bbox_inches='tight')\n",
    "\n",
    "    # Also create a bar chart of final importances\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    sorted_idx = np.argsort(importance_matrix[-1])\n",
    "    plt.barh(range(len(sorted_idx)), importance_matrix[-1, sorted_idx], align='center')\n",
    "    plt.yticks(range(len(sorted_idx)), [feature_names[i] for i in sorted_idx])\n",
    "    plt.title(f'Final Feature Importance ({max_trees} trees)')\n",
    "    plt.xlabel('Importance')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\"Final_Feature_Importance.png\", dpi=300, bbox_inches='tight')\n",
    "\n",
    "# Plot confusion matrix for model evaluation\n",
    "def plot_confusion_matrix(y_true, y_pred, title=\"Confusion Matrix\"):\n",
    "    \"\"\"\n",
    "    Plots a confusion matrix for binary classification results.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    y_true : array-like\n",
    "        True labels\n",
    "    y_pred : array-like\n",
    "        Predicted labels\n",
    "    title : str, optional\n",
    "        Title for the plot\n",
    "    \"\"\"\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=False)\n",
    "    plt.title(title)\n",
    "    plt.ylabel('True Label')\n",
    "    plt.xlabel('Predicted Label')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"{title.replace(' ', '_')}.png\", dpi=300, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e564f85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function will visualize feature importance and how it changes with trees\n",
    "def plot_feature_importance_evolution(X_train, y_train, feature_names, max_trees=100, intervals=5):\n",
    "    \"\"\"\n",
    "    Visualizes how feature importance evolves as the number of trees increases in a Random Forest.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    X_train : array-like\n",
    "        Training feature matrix\n",
    "    y_train : array-like\n",
    "        Training target vector\n",
    "    feature_names : list\n",
    "        Names of features\n",
    "    max_trees : int, optional (default=100)\n",
    "        Maximum number of trees to evaluate\n",
    "    intervals : int, optional (default=5)\n",
    "        Number of points to evaluate\n",
    "    \"\"\"\n",
    "    # Generate evenly spaced tree counts\n",
    "    tree_counts = np.linspace(5, max_trees, intervals, dtype=int)\n",
    "\n",
    "    # Initialize importance matrix\n",
    "    n_features = len(feature_names)\n",
    "    importance_matrix = np.zeros((len(tree_counts), n_features))\n",
    "\n",
    "    # For each tree count, train model and record feature importances\n",
    "    for i, n_trees in enumerate(tree_counts):\n",
    "        rf = RandomForestClassifier(n_estimators=n_trees, random_state=42)\n",
    "        rf.fit(X_train, y_train)\n",
    "        importance_matrix[i, :] = rf.feature_importances_\n",
    "\n",
    "    # Convert to DataFrame for easier visualization\n",
    "    importance_df = pd.DataFrame(importance_matrix,\n",
    "                               columns=feature_names,\n",
    "                               index=[f\"{n} trees\" for n in tree_counts])\n",
    "\n",
    "    # Get top 10 most important features (from final model)\n",
    "    top_features_idx = np.argsort(importance_matrix[-1])[-10:]\n",
    "    top_features = [feature_names[i] for i in top_features_idx]\n",
    "\n",
    "    # Plot heatmap of feature importance evolution for top features\n",
    "    plt.figure(figsize=(14, 8))\n",
    "    sns.heatmap(importance_df[top_features].T, annot=True, cmap='viridis', fmt='.3f')\n",
    "    plt.title('Feature Importance Evolution with Increasing Trees')\n",
    "    plt.xlabel('Number of Trees in Random Forest')\n",
    "    plt.ylabel('Features')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\"Feature_Importance_Evolution.png\", dpi=300, bbox_inches='tight')\n",
    "\n",
    "    # Also create a bar chart of final importances\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    sorted_idx = np.argsort(importance_matrix[-1])\n",
    "    plt.barh(range(len(sorted_idx)), importance_matrix[-1, sorted_idx], align='center')\n",
    "    plt.yticks(range(len(sorted_idx)), [feature_names[i] for i in sorted_idx])\n",
    "    plt.title(f'Final Feature Importance ({max_trees} trees)')\n",
    "    plt.xlabel('Importance')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\"Final_Feature_Importance.png\", dpi=300, bbox_inches='tight')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d68e1c1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot confusion matrix for model evaluation\n",
    "def plot_confusion_matrix(y_true, y_pred, title=\"Confusion Matrix\"):\n",
    "    \"\"\"\n",
    "    Plots a confusion matrix for binary classification results.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    y_true : array-like\n",
    "        True labels\n",
    "    y_pred : array-like\n",
    "        Predicted labels\n",
    "    title : str, optional\n",
    "        Title for the plot\n",
    "    \"\"\"\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', cbar=False)\n",
    "    plt.title(title)\n",
    "    plt.ylabel('True Label')\n",
    "    plt.xlabel('Predicted Label')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"{title.replace(' ', '_')}.png\", dpi=300, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0483e4b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to visualize threshold selection\n",
    "def plot_threshold_impact(y_true, y_scores):\n",
    "    \"\"\"\n",
    "    Visualizes the impact of different probability thresholds on precision, recall, and accuracy.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    y_true : array-like\n",
    "        True labels\n",
    "    y_scores : array-like\n",
    "        Predicted probability scores\n",
    "    \"\"\"\n",
    "    thresholds = np.linspace(0.01, 0.99, 50)\n",
    "    precision_scores = []\n",
    "    recall_scores = []\n",
    "    accuracy_scores = []\n",
    "    f1_scores = []\n",
    "\n",
    "    # Calculate metrics at different thresholds\n",
    "    for threshold in thresholds:\n",
    "        y_pred = (y_scores >= threshold).astype(int)\n",
    "\n",
    "        # True positives, false positives, etc.\n",
    "        tp = np.sum((y_true == 1) & (y_pred == 1))\n",
    "        fp = np.sum((y_true == 0) & (y_pred == 1))\n",
    "        tn = np.sum((y_true == 0) & (y_pred == 0))\n",
    "        fn = np.sum((y_true == 1) & (y_pred == 0))\n",
    "\n",
    "        # Calculate metrics\n",
    "        precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "        recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "        accuracy = (tp + tn) / (tp + tn + fp + fn)\n",
    "        f1 = 2 * precision * recall / (precision + recall) if (precision + recall) > 0 else 0\n",
    "\n",
    "        precision_scores.append(precision)\n",
    "        recall_scores.append(recall)\n",
    "        accuracy_scores.append(accuracy)\n",
    "        f1_scores.append(f1)\n",
    "\n",
    "    # Plot all metrics vs threshold\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.plot(thresholds, precision_scores, 'b-', label='Precision')\n",
    "    plt.plot(thresholds, recall_scores, 'g-', label='Recall')\n",
    "    plt.plot(thresholds, accuracy_scores, 'r-', label='Accuracy')\n",
    "    plt.plot(thresholds, f1_scores, 'y-', label='F1 Score')\n",
    "\n",
    "    # Find optimal F1 threshold\n",
    "    best_idx = np.argmax(f1_scores)\n",
    "    optimal_threshold = thresholds[best_idx]\n",
    "    plt.axvline(x=optimal_threshold, color='k', linestyle='--',\n",
    "              label=f'Optimal Threshold: {optimal_threshold:.2f}')\n",
    "\n",
    "    plt.title('Impact of Probability Threshold on Classification Metrics')\n",
    "    plt.xlabel('Threshold')\n",
    "    plt.ylabel('Score')\n",
    "    plt.legend(loc='best')\n",
    "    plt.grid(alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\"Threshold_Impact.png\", dpi=300, bbox_inches='tight')\n",
    "\n",
    "    return optimal_threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd7204ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize tree depth impact on performance\n",
    "def plot_max_depth_impact(X_train, y_train, X_test, y_test):\n",
    "    \"\"\"\n",
    "    Plots the impact of tree depth on model performance.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    X_train : array-like\n",
    "        Training feature matrix\n",
    "    y_train : array-like\n",
    "        Training target vector\n",
    "    X_test : array-like\n",
    "        Testing feature matrix\n",
    "    y_test : array-like\n",
    "        Testing target vector\n",
    "    \"\"\"\n",
    "    max_depths = np.arange(1, 21)\n",
    "    train_scores = []\n",
    "    test_scores = []\n",
    "\n",
    "    # For each max depth, train a model and record scores\n",
    "    for depth in max_depths:\n",
    "        rf = RandomForestClassifier(n_estimators=100, max_depth=depth, random_state=42)\n",
    "        rf.fit(X_train, y_train)\n",
    "\n",
    "        # Record scores\n",
    "        train_scores.append(accuracy_score(y_train, rf.predict(X_train)))\n",
    "        test_scores.append(accuracy_score(y_test, rf.predict(X_test)))\n",
    "\n",
    "    # Plot results\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(max_depths, train_scores, 'o-', color='r', label='Training Accuracy')\n",
    "    plt.plot(max_depths, test_scores, 'o-', color='g', label='Testing Accuracy')\n",
    "    plt.title('Effect of Tree Depth on Random Forest Performance')\n",
    "    plt.xlabel('Maximum Tree Depth')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.grid(alpha=0.3)\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(\"Tree_Depth_Impact.png\", dpi=300, bbox_inches='tight')\n",
    "\n",
    "    # Find optimal depth\n",
    "    best_idx = np.argmax(test_scores)\n",
    "    optimal_depth = max_depths[best_idx]\n",
    "    print(f\"Optimal tree depth: {optimal_depth}\")\n",
    "    return optimal_depth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5c2acd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main function to generate all visualizations\n",
    "def create_model_visualizations(ego_id, data_dir):\n",
    "    \"\"\"\n",
    "    Creates comprehensive visualizations of Random Forest model performance\n",
    "    for link prediction in an ego network.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    ego_id : str\n",
    "        ID of the ego network to analyze\n",
    "    data_dir : str\n",
    "        Path to the data directory\n",
    "    \"\"\"\n",
    "    print(f\"\\n======= Creating Model Visualizations for Ego Network {ego_id} =======\")\n",
    "\n",
    "    # === DATA LOADING ===\n",
    "    print(\"\\n[1/8] Loading network data...\")\n",
    "    # Load the network data (from original code)\n",
    "    G, circles = load_network(ego_id, data_dir)\n",
    "    node_features = extract_topological_features(G)\n",
    "    G_train, train_edges, test_edges, train_non_edges, test_non_edges = prepare_link_prediction_data(G)\n",
    "\n",
    "    # === DATASET CREATION ===\n",
    "    print(\"\\n[2/8] Creating feature datasets...\")\n",
    "    # Create training and testing datasets\n",
    "    X_train, y_train = create_edge_dataset(G_train, train_edges, train_non_edges, node_features)\n",
    "    X_test, y_test = create_edge_dataset(G_train, test_edges, test_non_edges, node_features)\n",
    "\n",
    "    # Scale features\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "    # Get feature names for visualization\n",
    "    sample_nodes = list(G.nodes())[:2]\n",
    "    feature_names = list(compute_edge_features(G, sample_nodes[0], sample_nodes[1], node_features).keys())\n",
    "\n",
    "    # === LEARNING CURVE VISUALIZATION ===\n",
    "    print(\"\\n[3/8] Generating learning curve...\")\n",
    "    # Shows how model performance changes with training set size\n",
    "    plot_learning_curve(\n",
    "        RandomForestClassifier(n_estimators=100, random_state=42),\n",
    "        X_train_scaled, y_train,\n",
    "        title=f\"Learning Curve - Ego Network {ego_id}\",\n",
    "        cv=5\n",
    "    )\n",
    "\n",
    "    # === NUMBER OF TREES VISUALIZATION ===\n",
    "    print(\"\\n[4/8] Analyzing impact of number of trees...\")\n",
    "    # Find optimal number of trees\n",
    "    optimal_trees = plot_n_estimators_curve(\n",
    "        X_train_scaled, y_train,\n",
    "        X_test_scaled, y_test,\n",
    "        max_n_estimators=150\n",
    "    )\n",
    "\n",
    "    # === TREE DEPTH IMPACT ===\n",
    "    print(\"\\n[5/8] Analyzing impact of tree depth...\")\n",
    "    # Find optimal tree depth\n",
    "    optimal_depth = plot_max_depth_impact(\n",
    "        X_train_scaled, y_train,\n",
    "        X_test_scaled, y_test\n",
    "    )\n",
    "\n",
    "    # === TRAIN OPTIMAL MODEL ===\n",
    "    print(\"\\n[6/8] Training optimal Random Forest model...\")\n",
    "    # Train the model with optimal parameters\n",
    "    best_rf = RandomForestClassifier(\n",
    "        n_estimators=optimal_trees,\n",
    "        max_depth=optimal_depth,\n",
    "        random_state=42\n",
    "    )\n",
    "    best_rf.fit(X_train_scaled, y_train)\n",
    "\n",
    "    # Make predictions\n",
    "    y_train_pred = best_rf.predict(X_train_scaled)\n",
    "    y_test_pred = best_rf.predict(X_test_scaled)\n",
    "    y_test_proba = best_rf.predict_proba(X_test_scaled)[:, 1]\n",
    "\n",
    "    # === CONFUSION MATRIX & METRICS ===\n",
    "    print(\"\\n[7/8] Creating performance evaluation visualizations...\")\n",
    "    # Plot confusion matrices\n",
    "    plot_confusion_matrix(y_train, y_train_pred, \"Training Confusion Matrix\")\n",
    "    plot_confusion_matrix(y_test, y_test_pred, \"Testing Confusion Matrix\")\n",
    "\n",
    "    # Plot threshold impact\n",
    "    optimal_threshold = plot_threshold_impact(y_test, y_test_proba)\n",
    "\n",
    "    # Apply optimal threshold\n",
    "    y_test_optimal = (y_test_proba >= optimal_threshold).astype(int)\n",
    "\n",
    "    # Print classification report\n",
    "    print(\"\\nClassification Report with Default Threshold (0.5):\")\n",
    "    print(classification_report(y_test, y_test_pred))\n",
    "\n",
    "    print(f\"\\nClassification Report with Optimal Threshold ({optimal_threshold:.2f}):\")\n",
    "    print(classification_report(y_test, y_test_optimal))\n",
    "\n",
    "    # === FEATURE IMPORTANCE EVOLUTION ===\n",
    "    print(\"\\n[8/8] Analyzing feature importance evolution...\")\n",
    "    # Visualize how feature importance evolves with more trees\n",
    "    plot_feature_importance_evolution(X_train_scaled, y_train, feature_names,\n",
    "                                     max_trees=optimal_trees, intervals=5)\n",
    "\n",
    "    print(\"\\nAll visualizations completed and saved!\")\n",
    "\n",
    "# These are helper functions that should be defined elsewhere in your code\n",
    "# I'm including placeholders for them to make the script work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec221d9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_network(ego_id, data_dir):\n",
    "    \"\"\"\n",
    "    Load the network data for a specific ego ID\n",
    "    \"\"\"\n",
    "    # Path to edge file\n",
    "    edge_file = os.path.join(data_dir, \"twitter\", f\"{ego_id}.edges\")\n",
    "\n",
    "    # Create a directed graph (since Twitter follows are directed)\n",
    "    G = nx.DiGraph()\n",
    "\n",
    "    # Add edges from the edge file\n",
    "    with open(edge_file, 'r') as f:\n",
    "        for line in f:\n",
    "            source, target = map(int, line.strip().split())\n",
    "            G.add_edge(source, target)\n",
    "\n",
    "    # Add the ego node (assumes ego follows everyone)\n",
    "    ego_node = int(ego_id)\n",
    "    # Create a copy of nodes before modifying the graph\n",
    "    nodes = list(G.nodes())\n",
    "    for node in nodes:\n",
    "        G.add_edge(ego_node, node)\n",
    "\n",
    "    # Load circles if available\n",
    "    circles = {}\n",
    "    circles_file = os.path.join(data_dir, \"twitter\", f\"{ego_id}.circles\")\n",
    "    if os.path.exists(circles_file):\n",
    "        with open(circles_file, 'r') as f:\n",
    "            for line in f:\n",
    "                parts = line.strip().split()\n",
    "                circle_name = parts[0]\n",
    "                circle_members = [int(x) for x in parts[1:]]\n",
    "                circles[circle_name] = circle_members\n",
    "\n",
    "    return G, circles\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "615c0dc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_topological_features(G):\n",
    "    \"\"\"\n",
    "    Extract topological features for each node\n",
    "    \"\"\"\n",
    "    # Get basic centrality measures\n",
    "    degree_centrality = nx.degree_centrality(G)\n",
    "    in_degree_centrality = nx.in_degree_centrality(G)\n",
    "    out_degree_centrality = nx.out_degree_centrality(G)\n",
    "\n",
    "    # PageRank as a measure of influence\n",
    "    try:\n",
    "        pagerank = nx.pagerank(G, alpha=0.85, max_iter=100)\n",
    "    except:\n",
    "        pagerank = {node: 0.0 for node in G.nodes()}\n",
    "\n",
    "    # Local clustering coefficient\n",
    "    try:\n",
    "        clustering = nx.clustering(G.to_undirected())\n",
    "    except:\n",
    "        clustering = {node: 0.0 for node in G.nodes()}\n",
    "\n",
    "    # Combine all features\n",
    "    node_features = {}\n",
    "    for node in G.nodes():\n",
    "        node_features[node] = {\n",
    "            'degree_centrality': degree_centrality.get(node, 0.0),\n",
    "            'in_degree_centrality': in_degree_centrality.get(node, 0.0),\n",
    "            'out_degree_centrality': out_degree_centrality.get(node, 0.0),\n",
    "            'pagerank': pagerank.get(node, 0.0),\n",
    "            'clustering': clustering.get(node, 0.0)\n",
    "        }\n",
    "\n",
    "    return node_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46b38a39",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_link_prediction_data(G, test_ratio=0.3, neg_ratio=1.0):\n",
    "    \"\"\"\n",
    "    Prepare data for link prediction\n",
    "    \"\"\"\n",
    "    # Extract all edges\n",
    "    all_edges = list(G.edges())\n",
    "\n",
    "    # Split edges into training and testing sets\n",
    "    train_edges, test_edges = train_test_split(all_edges, test_size=test_ratio, random_state=42)\n",
    "\n",
    "    # Create a new graph with only training edges\n",
    "    G_train = G.copy()\n",
    "    G_train.remove_edges_from(test_edges)\n",
    "\n",
    "    # Calculate negative edges (non-existing edges)\n",
    "    nodes = list(G.nodes())\n",
    "\n",
    "    # Function to sample negative edges\n",
    "    def sample_non_edges(graph, num_samples, exclude_edges=None):\n",
    "        if exclude_edges is None:\n",
    "            exclude_edges = set()\n",
    "        else:\n",
    "            exclude_edges = set(exclude_edges)\n",
    "\n",
    "        non_edges = []\n",
    "        while len(non_edges) < num_samples:\n",
    "            # Sample random node pairs\n",
    "            u, v = random.sample(nodes, 2)\n",
    "            # Check if this edge exists or is in excluded set\n",
    "            if u != v and not graph.has_edge(u, v) and (u, v) not in exclude_edges:\n",
    "                non_edges.append((u, v))\n",
    "                exclude_edges.add((u, v))\n",
    "\n",
    "        return non_edges\n",
    "\n",
    "    # Sample negative edges for training and testing\n",
    "    n_pos_train = len(train_edges)\n",
    "    n_pos_test = len(test_edges)\n",
    "\n",
    "    train_non_edges = sample_non_edges(G, int(n_pos_train * neg_ratio))\n",
    "    test_non_edges = sample_non_edges(G, int(n_pos_test * neg_ratio), exclude_edges=train_non_edges)\n",
    "\n",
    "    return G_train, train_edges, test_edges, train_non_edges, test_non_edges\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84ed5cb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_edge_features(G, node_u, node_v, node_features):\n",
    "    \"\"\"\n",
    "    Compute features for a potential edge between nodes u and v\n",
    "    \"\"\"\n",
    "    # Basic node features for both endpoints\n",
    "    u_feats = node_features.get(node_u, {})\n",
    "    v_feats = node_features.get(node_v, {})\n",
    "\n",
    "    # Common neighbors (in undirected sense)\n",
    "    G_undirected = G.to_undirected()\n",
    "    try:\n",
    "        common_neighbors = list(nx.common_neighbors(G_undirected, node_u, node_v))\n",
    "        num_common_neighbors = len(common_neighbors)\n",
    "    except:\n",
    "        num_common_neighbors = 0\n",
    "\n",
    "    # Preferential attachment score (product of degrees)\n",
    "    pref_attachment = G_undirected.degree(node_u) * G_undirected.degree(node_v)\n",
    "\n",
    "    # Jaccard coefficient\n",
    "    try:\n",
    "        u_neighbors = set(G_undirected.neighbors(node_u))\n",
    "        v_neighbors = set(G_undirected.neighbors(node_v))\n",
    "        if len(u_neighbors | v_neighbors) > 0:\n",
    "            jaccard = len(u_neighbors & v_neighbors) / len(u_neighbors | v_neighbors)\n",
    "        else:\n",
    "            jaccard = 0.0\n",
    "    except:\n",
    "        jaccard = 0.0\n",
    "\n",
    "    # For directed graph metrics\n",
    "    try:\n",
    "        u_successors = set(G.successors(node_u))\n",
    "        u_predecessors = set(G.predecessors(node_u))\n",
    "        v_successors = set(G.successors(node_v))\n",
    "        v_predecessors = set(G.predecessors(node_v))\n",
    "\n",
    "        # Reciprocity: if v follows u already\n",
    "        reciprocity = 1.0 if node_u in v_successors else 0.0\n",
    "\n",
    "        # Follower overlap\n",
    "        follower_overlap = len(u_predecessors & v_predecessors) / max(1, min(len(u_predecessors), len(v_predecessors)))\n",
    "\n",
    "        # Following overlap\n",
    "        following_overlap = len(u_successors & v_successors) / max(1, min(len(u_successors), len(v_successors)))\n",
    "    except:\n",
    "        reciprocity = 0.0\n",
    "        follower_overlap = 0.0\n",
    "        following_overlap = 0.0\n",
    "\n",
    "    # Adamic-Adar index (weighted common neighbors)\n",
    "    adamic_adar = 0.0\n",
    "    try:\n",
    "        for common_neighbor in common_neighbors:\n",
    "            neighbor_degree = G_undirected.degree(common_neighbor)\n",
    "            if neighbor_degree > 1:  # Avoid log(1) = 0 division\n",
    "                adamic_adar += 1.0 / np.log(neighbor_degree)\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "    # Combine all edge features\n",
    "    edge_features = {\n",
    "        # Node features for both endpoints\n",
    "        'u_degree': u_feats.get('degree_centrality', 0.0),\n",
    "        'u_in_degree': u_feats.get('in_degree_centrality', 0.0),\n",
    "        'u_out_degree': u_feats.get('out_degree_centrality', 0.0),\n",
    "        'u_pagerank': u_feats.get('pagerank', 0.0),\n",
    "        'u_clustering': u_feats.get('clustering', 0.0),\n",
    "\n",
    "        'v_degree': v_feats.get('degree_centrality', 0.0),\n",
    "        'v_in_degree': v_feats.get('in_degree_centrality', 0.0),\n",
    "        'v_out_degree': v_feats.get('out_degree_centrality', 0.0),\n",
    "        'v_pagerank': v_feats.get('pagerank', 0.0),\n",
    "        'v_clustering': v_feats.get('clustering', 0.0),\n",
    "\n",
    "        # Edge-specific features\n",
    "        'common_neighbors': num_common_neighbors,\n",
    "        'preferential_attachment': pref_attachment,\n",
    "        'jaccard_coefficient': jaccard,\n",
    "        'adamic_adar': adamic_adar,\n",
    "        'reciprocity': reciprocity,\n",
    "        'follower_overlap': follower_overlap,\n",
    "        'following_overlap': following_overlap\n",
    "    }\n",
    "\n",
    "    return edge_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72f69c10",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_edge_dataset(G, pos_edges, neg_edges, node_features):\n",
    "    \"\"\"\n",
    "    Create a dataset for edge prediction\n",
    "    \"\"\"\n",
    "    X = []\n",
    "    y = []\n",
    "\n",
    "    # Add positive examples\n",
    "    for u, v in pos_edges:\n",
    "        edge_feats = compute_edge_features(G, u, v, node_features)\n",
    "        X.append(list(edge_feats.values()))\n",
    "        y.append(1)  # Positive class\n",
    "\n",
    "    # Add negative examples\n",
    "    for u, v in neg_edges:\n",
    "        edge_feats = compute_edge_features(G, u, v, node_features)\n",
    "        X.append(list(edge_feats.values()))\n",
    "        y.append(0)  # Negative class\n",
    "\n",
    "    return np.array(X), np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ef63290",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # URL of the dataset\n",
    "    url = \"https://snap.stanford.edu/data/twitter.tar.gz\"\n",
    "    data_dir = \"./data\"\n",
    "\n",
    "    # Download and extract the dataset\n",
    "    download_data(url, data_dir)\n",
    "\n",
    "    # List available ego networks\n",
    "    ego_networks = []\n",
    "    for filename in os.listdir(os.path.join(data_dir, \"twitter\")):\n",
    "        if filename.endswith(\".edges\"):\n",
    "            ego_networks.append(filename.split(\".\")[0])\n",
    "\n",
    "    print(f\"Found {len(ego_networks)} ego networks: {ego_networks}\")\n",
    "\n",
    "    # Select a single ego network to analyze\n",
    "    if len(ego_networks) > 0:\n",
    "        # You can change this index to select a different network\n",
    "        network_index = 0\n",
    "        ego_id = ego_networks[network_index]\n",
    "\n",
    "        try:\n",
    "            # Run link prediction\n",
    "            results = run_link_prediction(ego_id, data_dir)\n",
    "\n",
    "            # Print a summary of the results\n",
    "            print(\"\\nLink Prediction Results Summary:\")\n",
    "            print(f\"ROC AUC Score: {results['auc_score']:.4f}\")\n",
    "            print(f\"Average Precision Score: {results['ap_score']:.4f}\")\n",
    "\n",
    "            print(\"\\nTop 5 Most Important Features:\")\n",
    "            sorted_features = sorted(results['feature_importance'].items(), key=lambda x: x[1], reverse=True)\n",
    "            for feature, importance in sorted_features[:5]:\n",
    "                print(f\"  - {feature}: {importance:.4f}\")\n",
    "\n",
    "            # Plot training and testing accuracy vs. number of trees\n",
    "            plt.figure(figsize=(10, 6))\n",
    "            plt.plot(results['n_estimators_list'], results['train_scores'], 'o-', label='Training Accuracy')\n",
    "            plt.plot(results['n_estimators_list'], results['test_scores'], 'o-', label='Testing Accuracy')\n",
    "            plt.plot(results['n_estimators_list'], results['oob_scores'], 'o-', label='Out-of-Bag Accuracy')\n",
    "\n",
    "            plt.xlabel('Number of Trees in Random Forest')\n",
    "            plt.ylabel('Accuracy')\n",
    "            plt.title('Learning Curve: Model Performance vs. Ensemble Size')\n",
    "            plt.grid(alpha=0.3)\n",
    "            plt.legend()\n",
    "\n",
    "            plt.tight_layout()\n",
    "            plt.savefig(f\"learning_curve_ego_{ego_id}.png\", dpi=300, bbox_inches='tight')\n",
    "            plt.show()\n",
    "\n",
    "        except Exception as e:\n",
    "            print(\"\\nAn error occurred during link prediction:\")\n",
    "            print(str(e))\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "    else:\n",
    "        print(\"No ego networks found. Check the dataset extraction.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
